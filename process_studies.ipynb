{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNkii2B7H4ZpktZ4ZdxTMhf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "377c9de3261e42bfb3f90992ddd2752c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6a49f0d9f6724ceb8f3c5f28c579f300",
              "IPY_MODEL_0e4a4955990d42a5a632590d4da457fd",
              "IPY_MODEL_1dffc236762143d09a64846b1aebf1a0"
            ],
            "layout": "IPY_MODEL_20883d8c46804cec9d9a7606e85421f6"
          }
        },
        "6a49f0d9f6724ceb8f3c5f28c579f300": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32e20cf5ec884bbdba607a60a0deb02d",
            "placeholder": "​",
            "style": "IPY_MODEL_19f17559a9ba4364a808bf53ca8ded3b",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "0e4a4955990d42a5a632590d4da457fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eeda2a992c974ea0b08660a4270dfcbb",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9c15834dd01149fba3153a19d6301421",
            "value": 2
          }
        },
        "1dffc236762143d09a64846b1aebf1a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_53862a39ddd64d03805ce7deb1992683",
            "placeholder": "​",
            "style": "IPY_MODEL_a7000cc149ad429c974d6d02f07a6b43",
            "value": " 2/2 [00:21&lt;00:00, 10.36s/it]"
          }
        },
        "20883d8c46804cec9d9a7606e85421f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32e20cf5ec884bbdba607a60a0deb02d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19f17559a9ba4364a808bf53ca8ded3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eeda2a992c974ea0b08660a4270dfcbb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c15834dd01149fba3153a19d6301421": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "53862a39ddd64d03805ce7deb1992683": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7000cc149ad429c974d6d02f07a6b43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alex-jk/ExData_Plotting1/blob/master/process_studies.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This notebook extracts and cleans text from PDF reports for use in training the buyer profile extractor model.**"
      ],
      "metadata": {
        "id": "h3140H82TCGw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Add input and output examples to jsonl file**"
      ],
      "metadata": {
        "id": "aSaUOBcBZMvG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!git clone https://github.com/alex-jk/buyers-ID-manual-project.git\n",
        "%cd buyers-ID-manual-project\n",
        "!ls"
      ],
      "metadata": {
        "id": "Gi3MKqB7cGcL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec92c027-c0e3-4562-b02c-bf21e2c6a5a6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'buyers-ID-manual-project'\n",
            "/content/buyers-ID-manual-project\n",
            "data\t\t     LICENSE\t\t    __pycache__\n",
            "extraction_utils.py  process_studies.ipynb  README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import json\n",
        "\n",
        "# input_text = \"\"\"\n",
        "# Buyers were willing to travel significant distances to reach underage victims.\n",
        "# One man cited the Atlanta airport as a regular meeting place.\n",
        "# \"\"\"\n",
        "\n",
        "# output_text = \"\"\"\n",
        "# Buyers are often mobile and willing to travel, including to major hubs like the Atlanta airport.\n",
        "# \"\"\"\n",
        "\n",
        "# entry = {\"input\": input_text.strip(), \"output\": output_text.strip()}\n",
        "\n",
        "# with open(\"data/labeled_chunks.jsonl\", \"a\", encoding=\"utf-8\") as f:\n",
        "#     f.write(json.dumps(entry) + \"\\n\")"
      ],
      "metadata": {
        "id": "-l9bAG6lTEYW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import libraries**"
      ],
      "metadata": {
        "id": "FHHdr7ZZTYe3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install specific versions targeting CUDA 12.1 (likely compatible with 12.5 drivers)\n",
        "!pip install torch==2.3.1+cu121 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "# Reinstall the other pinned versions\n",
        "!pip install transformers==4.41.2 accelerate==0.31.0 --no-deps\n",
        "\n",
        "# Ensure base dependencies are present\n",
        "!pip install bitsandbytes sentencepiece\n",
        "\n",
        "print(\"Installed specific package versions (torch targeting cu121). PLEASE RESTART RUNTIME NOW.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTTOXwDwcuaQ",
        "outputId": "e118691d-a7e5-4fef-b10c-7ecb65ba475e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==2.3.1+cu121\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torch-2.3.1%2Bcu121-cp311-cp311-linux_x86_64.whl (781.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.0/781.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.3.1+cu121) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.1+cu121) (4.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.3.1+cu121) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.3.1+cu121) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.1+cu121) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.3.1+cu121) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.3.1+cu121)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.3.1+cu121)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.3.1+cu121)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.3.1+cu121)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.3.1+cu121)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.3.1+cu121)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.3.1+cu121)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.3.1+cu121)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.3.1+cu121)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.3.1+cu121)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.3.1+cu121)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.3.1 (from torch==2.3.1+cu121)\n",
            "  Using cached triton-2.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.1+cu121) (12.4.127)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.3.1+cu121) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.3.1+cu121) (1.3.0)\n",
            "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Using cached triton-2.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n",
            "Installing collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.3.1.170\n",
            "    Uninstalling nvidia-cusparse-cu12-12.3.1.170:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.3.1.170\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.5.147\n",
            "    Uninstalling nvidia-curand-cu12-10.3.5.147:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.5.147\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.1.3\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.1.3:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.1.3\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.4.127\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.4.127\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.4.127\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.4.5.8\n",
            "    Uninstalling nvidia-cublas-cu12-12.4.5.8:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.4.5.8\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.1.9\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.1.9:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.1.9\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.1.0.70\n",
            "    Uninstalling nvidia-cudnn-cu12-9.1.0.70:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.1.0.70\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.3.1\n",
            "    Uninstalling torch-2.3.1:\n",
            "      Successfully uninstalled torch-2.3.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.3.1+cu121 which is incompatible.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.3.1+cu121 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvtx-cu12-12.1.105 torch-2.3.1+cu121 triton-2.3.1\n",
            "Requirement already satisfied: transformers==4.41.2 in /usr/local/lib/python3.11/dist-packages (4.41.2)\n",
            "Requirement already satisfied: accelerate==0.31.0 in /usr/local/lib/python3.11/dist-packages (0.31.0)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.3.1+cu121)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Installed specific package versions (torch targeting cu121). PLEASE RESTART RUNTIME NOW.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import sys\n",
        "import os"
      ],
      "metadata": {
        "id": "yCeel7fiPtID"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Read input output file and write to jsonl**"
      ],
      "metadata": {
        "id": "YPvesEjPT4au"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_output_df = pd.read_csv(\"data/buyers_id_manual_input_output.csv\")\n",
        "\n",
        "print(input_output_df.shape)\n",
        "print(input_output_df.columns)\n",
        "print(\"\\n-------------------------------\\n\")\n",
        "print(input_output_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9u-YHG4rTcu7",
        "outputId": "f01740f2-a74b-47c5-f20a-ff59ad0209d5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(9, 3)\n",
            "Index(['Study', 'Input', 'Output'], dtype='object')\n",
            "\n",
            "-------------------------------\n",
            "\n",
            "                                    Study  \\\n",
            "0  The Shapiro Group Georgia Demand Study   \n",
            "1  The Shapiro Group Georgia Demand Study   \n",
            "2  The Shapiro Group Georgia Demand Study   \n",
            "3  The Shapiro Group Georgia Demand Study   \n",
            "4  The Shapiro Group Georgia Demand Study   \n",
            "\n",
            "                                               Input  \\\n",
            "0  Almost half these men are the age 30-39, with ...   \n",
            "1  The data clearly debunk the myth that CSEC is ...   \n",
            "2  Not only are 65% of men who buy sex with young...   \n",
            "3  Craigslist is by far the most efficient medium...   \n",
            "4  While many of the men who exploit these childr...   \n",
            "\n",
            "                                              Output  \n",
            "0  Almost half these men are the age 30-39, with ...  \n",
            "1  Men who\\nrespond to advertisements for sex wit...  \n",
            "2  65% of men who buy sex with young females do s...  \n",
            "3  Buyers respond to Craigslist ads 3 times more ...  \n",
            "4  Nearly half of buyers are willing to pay for s...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_output_df = input_output_df.drop_duplicates(subset=[\"Study\", \"Input\", \"Output\"])\n",
        "\n",
        "# Convert to JSONL\n",
        "with open(\"data/labeled_chunks.jsonl\", \"w\") as f:\n",
        "    for _, row in input_output_df.iterrows():\n",
        "        json_obj = {\n",
        "            \"study\": row[\"Study\"],\n",
        "            \"input\": row[\"Input\"],\n",
        "            \"output\": row[\"Output\"]\n",
        "        }\n",
        "        f.write(json.dumps(json_obj) + \"\\n\")"
      ],
      "metadata": {
        "id": "jC56MIoNVPMZ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Models for text extraction**"
      ],
      "metadata": {
        "id": "z7s-W-gNZPi8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from extraction_utils import create_prompt_messages, extract_information\n",
        "\n",
        "print(\"Functions 'create_prompt_messages' and 'extract_information' imported successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 494
        },
        "id": "sRHpTCINVOxX",
        "outputId": "37b42080-fef8-4597-cfbe-7aca50ad5226"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "libcudnn.so.8: cannot open shared object file: No such file or directory",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-b6884620306c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mextraction_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_prompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract_information\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Functions 'create_prompt_messages' and 'extract_information' imported successfully.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/buyers-ID-manual-project/extraction_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# --- Define the Prompt (Few-Shot Example) ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_prompt_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_chunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m\"\"\"Creates the chat message structure expected by Phi-3.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mUSE_GLOBAL_DEPS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0m_load_global_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;31m# Appease the type checker; ordinarily this binding is inserted by the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: libcudnn.so.8: cannot open shared object file: No such file or directory",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgJ0s70ynDtG",
        "outputId": "282d498c-7007-464c-c88a-85f7f47e990e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
      ],
      "metadata": {
        "id": "YYtd4ZYUZRw2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration ---\n",
        "model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "\n",
        "# --- Load Model and Tokenizer ---\n",
        "# Load the model with 4-bit quantization to save memory (requires bitsandbytes)\n",
        "# Use device_map=\"auto\" to automatically use GPU if available\n",
        "try:\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        device_map=\"auto\",         # Use GPU if available, otherwise CPU\n",
        "        torch_dtype=\"auto\",        # Automatically select appropriate dtype\n",
        "        trust_remote_code=True,    # Phi-3 requires this\n",
        "        # Optional: uncomment below for 4-bit loading (needs bitsandbytes)\n",
        "        # load_in_4bit=True,\n",
        "        # bnb_4bit_compute_dtype=torch.bfloat16\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    print(f\"Model '{model_id}' loaded successfully.\")\n",
        "\n",
        "    # --- Create a Hugging Face Pipeline for easier text generation ---\n",
        "    # Note: max_new_tokens controls how long the generated output can be. Adjust as needed.\n",
        "    pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        # Adjust max_new_tokens if your extracted text might be longer\n",
        "        # Needs to be long enough for the longest expected extraction + \"NONE\"\n",
        "        max_new_tokens=256,\n",
        "        # Temperature=0 means more deterministic output, higher means more creative/random\n",
        "        # temperature=0.0,\n",
        "        # top_p=0.95, # Optional: nucleus sampling\n",
        "        do_sample=False # Set to False for more deterministic extraction\n",
        "    )\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model or creating pipeline: {e}\")\n",
        "    print(\"Ensure you have sufficient RAM/VRAM and necessary libraries installed.\")\n",
        "    print(\"Consider using Google Colab with a T4 GPU runtime.\")\n",
        "    # Exit if model loading fails\n",
        "    exit()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "377c9de3261e42bfb3f90992ddd2752c",
            "6a49f0d9f6724ceb8f3c5f28c579f300",
            "0e4a4955990d42a5a632590d4da457fd",
            "1dffc236762143d09a64846b1aebf1a0",
            "20883d8c46804cec9d9a7606e85421f6",
            "32e20cf5ec884bbdba607a60a0deb02d",
            "19f17559a9ba4364a808bf53ca8ded3b",
            "eeda2a992c974ea0b08660a4270dfcbb",
            "9c15834dd01149fba3153a19d6301421",
            "53862a39ddd64d03805ce7deb1992683",
            "a7000cc149ad429c974d6d02f07a6b43"
          ]
        },
        "id": "86Z4-8tTnLC_",
        "outputId": "f6fde951-e4f0-4d04-efc8-bf6039e11d56"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "377c9de3261e42bfb3f90992ddd2752c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model 'microsoft/Phi-3-mini-4k-instruct' loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example Usage**"
      ],
      "metadata": {
        "id": "XywDoaLuZcBU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text_1 = \"Almost half these men are the age 30-39, with the next largest group being men under age 30. The mean age is 33 and the median 31. The youngest survey participant was 18, and the oldest was 67.\"\n",
        "\n",
        "input_text_2 = \"The data clearly debunk the myth that CSEC is a problem relegated to the urban core. Men who respond to advertisements for sex with young females come from all over metro Atlanta, the geographic market where the advertisements in this study were targeted.\"\n",
        "\n",
        "input_text_3 = \"This paragraph discusses unrelated economic factors in the region and contains no information about buyers or traffickers.\"\n",
        "\n",
        "input_text_4 = \"Research indicates traffickers often groom potential buyers by displaying luxury goods online and frequenting specific forums known for risky behavior discussions. Victims may appear withdrawn or display signs of coaching. Reporting suspicions anonymously through the national hotline is encouraged.\""
      ],
      "metadata": {
        "id": "uVOQqRnEZeOg"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Extracting Information ---\")\n",
        "\n",
        "# Check if the pipeline object exists before trying to use it\n",
        "if 'pipe' in locals() and pipe is not None:\n",
        "    print(f\"\\nInput 1:\\n{input_text_1}\")\n",
        "    extraction_1 = extract_information(input_text_1, pipe)\n",
        "    print(f\"\\nExtraction 1:\\n{extraction_1}\")\n",
        "\n",
        "    print(\"-\" * 20)\n",
        "\n",
        "    print(f\"\\nInput 2:\\n{input_text_2}\")\n",
        "    extraction_2 = extract_information(input_text_2, pipe)\n",
        "    print(f\"\\nExtraction 2:\\n{extraction_2}\")\n",
        "\n",
        "    print(\"-\" * 20)\n",
        "\n",
        "    print(f\"\\nInput 3:\\n{input_text_3}\")\n",
        "    extraction_3 = extract_information(input_text_3, pipe)\n",
        "    print(f\"\\nExtraction 3:\\n{extraction_3}\")\n",
        "\n",
        "    print(\"-\" * 20)\n",
        "\n",
        "    print(f\"\\nInput 4:\\n{input_text_4}\")\n",
        "    extraction_4 = extract_information(input_text_4, pipe)\n",
        "    print(f\"\\nExtraction 4:\\n{extraction_4}\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nERROR: The 'pipe' object (text generation pipeline) was not found or not created successfully.\")\n",
        "    print(\"Please ensure the model loading cell was run successfully after importing functions.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ju44xbqscRuN",
        "outputId": "aee5b54c-ab3d-459f-fc9a-65199df4bb81"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Extracting Information ---\n",
            "\n",
            "Input 1:\n",
            "Almost half these men are the age 30-39, with the next largest group being men under age 30. The mean age is 33 and the median 31. The youngest survey participant was 18, and the oldest was 67.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error during generation: 'DynamicCache' object has no attribute 'get_max_length'\n",
            "\n",
            "Extraction 1:\n",
            "Error: Could not generate output. ('DynamicCache' object has no attribute 'get_max_length')\n",
            "--------------------\n",
            "\n",
            "Input 2:\n",
            "The data clearly debunk the myth that CSEC is a problem relegated to the urban core. Men who respond to advertisements for sex with young females come from all over metro Atlanta, the geographic market where the advertisements in this study were targeted.\n",
            "Error during generation: 'DynamicCache' object has no attribute 'get_max_length'\n",
            "\n",
            "Extraction 2:\n",
            "Error: Could not generate output. ('DynamicCache' object has no attribute 'get_max_length')\n",
            "--------------------\n",
            "\n",
            "Input 3:\n",
            "This paragraph discusses unrelated economic factors in the region and contains no information about buyers or traffickers.\n",
            "Error during generation: 'DynamicCache' object has no attribute 'get_max_length'\n",
            "\n",
            "Extraction 3:\n",
            "Error: Could not generate output. ('DynamicCache' object has no attribute 'get_max_length')\n",
            "--------------------\n",
            "\n",
            "Input 4:\n",
            "Research indicates traffickers often groom potential buyers by displaying luxury goods online and frequenting specific forums known for risky behavior discussions. Victims may appear withdrawn or display signs of coaching. Reporting suspicions anonymously through the national hotline is encouraged.\n",
            "Error during generation: 'DynamicCache' object has no attribute 'get_max_length'\n",
            "\n",
            "Extraction 4:\n",
            "Error: Could not generate output. ('DynamicCache' object has no attribute 'get_max_length')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check installed versions of required packages\n",
        "!pip list | grep -E 'transformers|torch|accelerate|flash-attn'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMiPjqyUfUK8",
        "outputId": "9da07e04-504b-4f01-cf0e-bd6659527ba5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accelerate                            1.6.0\n",
            "sentence-transformers                 3.4.1\n",
            "torch                                 2.6.0+cu124\n",
            "torchaudio                            2.6.0+cu124\n",
            "torchsummary                          1.5.1\n",
            "torchvision                           0.21.0+cu124\n",
            "transformers                          4.51.3\n"
          ]
        }
      ]
    }
  ]
}